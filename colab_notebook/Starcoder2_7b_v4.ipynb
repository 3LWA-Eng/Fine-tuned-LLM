{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# Util code : Restart the kernel\n",
        "# ============================================================================\n",
        "import IPython\n",
        "#IPython.Application.instance().kernel.do_shutdown(True)"
      ],
      "metadata": {
        "id": "LeZ4HUq19x8j"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# Install Required Libraries\n",
        "# ============================================================================\n",
        "!pip install -q --upgrade bitsandbytes accelerate transformers"
      ],
      "metadata": {
        "id": "BVUUZbTA96V2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# Import Libraries\n",
        "# ============================================================================\n",
        "import torch\n",
        "import json\n",
        "from IPython.display import Markdown, display, update_display\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "import os\n",
        "# List all notebooks in the folder\n",
        "notebook_folder = '/content/drive/MyDrive/Colab_Notebooks'\n",
        "os.listdir(notebook_folder)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N71k9iow97hO",
        "outputId": "091d72da-2cc8-4a0e-f522-6860ce178c96"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Untitled0.ipynb',\n",
              " 'Copy of Preprocessing.ipynb',\n",
              " 'Untitled',\n",
              " 'Untitled1.ipynb',\n",
              " 'Untitled2.ipynb',\n",
              " 'Untitled3.ipynb',\n",
              " 'Untitled4.ipynb',\n",
              " 'Copy of starcoder2-3b.ipynb',\n",
              " 'starcoder2-7b.ipynb',\n",
              " 'Dataset.json',\n",
              " 'Output_MultiShot.json',\n",
              " 'IR_NLTK_Preprocessing (1).ipynb',\n",
              " 'Output_ZeroShot.json',\n",
              " 'Starcoder2-7b v4.ipynb']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# Load StarCoder2 Model\n",
        "# ============================================================================\n",
        "\n",
        "# Quantization Config - this allows us to load the model into memory and use less memory\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, # quantization for huge memory saving\n",
        "    bnb_4bit_use_double_quant=True, # double quantization for a bit more memory saving\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16, # std good practice\n",
        "    bnb_4bit_quant_type=\"nf4\" # minor perfromance improvement\n",
        ")\n",
        "\n",
        "model_name = \"bigcode/starcoder2-7b\"\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token # std good practice\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\"  # Better than .to(\"cuda\") for quantized models\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "8a048d8036304b47926cb89c6fdec0d5",
            "a125acdb57f8494cbe2959b8cf08411d",
            "36ed13551e8b42e5b8f0ec5cd4413436",
            "339bdb485fe34a8295e16cd2eb87f935",
            "9dab811424454f8e9872805b730445e3",
            "9f7f46454e6f433183d33142721c4510",
            "4941fde539814ed1909c42744b82fd7b",
            "70b568e72e8d49309344c3b7b354df3e",
            "14f42f5df5184e739b7ab36efa4878ce",
            "82fb83a33c1d45f7b1c4ed9e74ba18d1",
            "66629080bb564b48aa86d707ba4125ef"
          ]
        },
        "id": "Lld9_JoL-HLn",
        "outputId": "1025bc3d-f430-41f7-a7be-3b93c6d9d7a2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a048d8036304b47926cb89c6fdec0d5"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# Generate Function\n",
        "# ============================================================================\n",
        "def generate(full_prompt):\n",
        "  # tokenize the prompt (no chat template)\n",
        "  input_ids = tokenizer(full_prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "  attention_mask = torch.ones_like(input_ids, dtype=torch.long, device=\"cuda\")\n",
        "\n",
        "  outputs = model.generate(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask,\n",
        "      max_new_tokens=128,\n",
        "      temperature=0.2,  # lower for consistency\n",
        "      top_p=0.9,  # Tighter sampling\n",
        "      do_sample=True,  # Required when using temperature\n",
        "      pad_token_id=tokenizer.eos_token_id\n",
        "  )\n",
        "  full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "  # Extract only the newly generated explanation\n",
        "  response = full_text.split(\"Output:\")[-1].split(\"---\")[0].strip()\n",
        "\n",
        "  return response\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MeebukKB-JNM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset from Drive\n",
        "with open('/content/drive/MyDrive/Colab_Notebooks/Dataset.json', 'r') as f:\n",
        "    dataset = json.load(f)"
      ],
      "metadata": {
        "id": "hDyO-m5p-UQm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# Create Prompt Function\n",
        "# ============================================================================\n",
        "def create_prompt(sample, examples=0):\n",
        "\n",
        "    # Enhanced system message for code generation\n",
        "    prompt = \"\"\"You are an expert code assistant. Generate accurate, clean, and efficient code.\n",
        "\n",
        "Rules:\n",
        "- Provide ONLY the code solution\n",
        "- No explanations before or after the code\n",
        "- Use proper indentation and formatting\n",
        "- Follow best practices\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "    # Add examples if provided (n-shot)\n",
        "    if examples > 0:\n",
        "        prompt += \"Here are example solutions:\\n\\n\"\n",
        "\n",
        "        # Collect example demonstrations (avoid using the test sample itself)\n",
        "        samples = []\n",
        "        for item in dataset:\n",
        "            # Skip if this is the current test sample\n",
        "            if item.get(\"id\") == sample.get(\"id\"):\n",
        "                continue\n",
        "\n",
        "            samples.append({\n",
        "                \"instruction\": item[\"instruction\"],\n",
        "                \"input\": item[\"input\"],\n",
        "                \"output\": item[\"output\"]\n",
        "            })\n",
        "\n",
        "            if len(samples) >= examples:\n",
        "                break\n",
        "\n",
        "        # Add example demonstrations with clear separators\n",
        "        for i, ex in enumerate(samples, 1):\n",
        "            prompt += f\"\"\"Instruction: {ex[\"instruction\"]}\n",
        "Input:\n",
        "{ex[\"input\"]}\n",
        "Output:\n",
        "{ex[\"output\"]}\n",
        "\n",
        "---\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "        # Add the actual query (without answer)\n",
        "        prompt += f\"\"\"Now solve this:\n",
        "\n",
        "Instruction: {sample[\"instruction\"]}\n",
        "Input:\n",
        "{sample[\"input\"]}\n",
        "Output:\n",
        "\"\"\"\n",
        "    else:\n",
        "        # 0-shot - direct task\n",
        "        prompt += f\"\"\"Instruction: {sample[\"instruction\"]}\n",
        "Input:\n",
        "{sample[\"input\"]}\n",
        "Output:\n",
        "\"\"\"\n",
        "\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "nWzGBGDW-YOL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# Zero-Shot Prompt\n",
        "# ============================================================================\n",
        "def ZeroShot(max_tests=110):\n",
        "  print(\"\\n\" + \"=\"*80)\n",
        "  print(\"StarCoder2 Zero-Shot Prompting\")\n",
        "  print(\"=\"*80)\n",
        "  results = []\n",
        "  tests_processed = 0\n",
        "\n",
        "  # Inference Loop\n",
        "  for item in dataset:\n",
        "      # Stop when we've processed enough tests\n",
        "      if tests_processed >= max_tests:\n",
        "          break\n",
        "\n",
        "      prediction = generate(create_prompt(item))\n",
        "      results.append({\n",
        "          \"id\": item[\"id\"],\n",
        "          \"input\": item[\"input\"],\n",
        "          \"expected\": item[\"output\"],\n",
        "          \"actual\": prediction\n",
        "      })\n",
        "      print(f\"Processed ID: {item['id']} ({tests_processed + 1}/{max_tests})\")\n",
        "      tests_processed += 1\n",
        "\n",
        "  print(f\"\\nTotal tests processed: {tests_processed}\")\n",
        "\n",
        "\n",
        "  # Save to Drive\n",
        "  with open('/content/drive/MyDrive/Colab_Notebooks/Output_ZeroShot.json', 'w') as f:\n",
        "      json.dump(results, f, indent=4)"
      ],
      "metadata": {
        "id": "LDvPvlN6-eMH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# Multi-Shot Prompting\n",
        "# ============================================================================\n",
        "def MultiShot(nShots, max_tests=150):\n",
        "    \"\"\"\n",
        "    Performs multi-shot prompting on the dataset.\n",
        "\n",
        "    Args:\n",
        "        nShots: Number of examples to include in the prompt (0 for zero-shot)\n",
        "        max_tests: Maximum number of test samples to process (default: 10)\n",
        "\n",
        "    Returns:\n",
        "        None (saves results to file)\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"StarCoder2 Multi-Shot Prompting\")\n",
        "    print(\"=\"*80)\n",
        "    results = []\n",
        "    tests_processed = 0\n",
        "\n",
        "    # Inference Loop\n",
        "    for item in dataset:\n",
        "        # Skip samples used as examples\n",
        "        if item[\"id\"] <= nShots:\n",
        "            continue\n",
        "\n",
        "        # Stop when we've processed enough tests\n",
        "        if tests_processed >= max_tests:\n",
        "            break\n",
        "\n",
        "        prediction = generate(create_prompt(item, nShots))\n",
        "        results.append({\n",
        "            \"id\": item[\"id\"],\n",
        "            \"input\": item[\"input\"],\n",
        "            \"expected\": item[\"output\"],\n",
        "            \"actual\": prediction\n",
        "        })\n",
        "        print(f\"Processed ID: {item['id']} ({tests_processed + 1}/{max_tests})\")\n",
        "        tests_processed += 1\n",
        "\n",
        "    print(f\"\\nTotal tests processed: {tests_processed}\")\n",
        "\n",
        "    # Save to Drive\n",
        "    with open('/content/drive/MyDrive/Colab_Notebooks/Output_MultiShot.json', 'w') as f:\n",
        "        json.dump(results, f, indent=4)\n",
        "\n",
        "    print(\"Results saved to Output_MultiShot.json\")\n"
      ],
      "metadata": {
        "id": "3DqcnlTY-n0A"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_custom(input_text=\"\", nShots=0):\n",
        "    # We do a manual sample\n",
        "    sample = {\n",
        "        \"id\": 10**9,  # A large number so that it is impossible to collide with IDs in the dataset\n",
        "        \"instruction\": \"Summarize the purpose of this C++ code in one or two sentences.\",\n",
        "        \"input\": input_text,\n",
        "        \"output\": \"\"\n",
        "    }\n",
        "\n",
        "    # We use create_prompt as it is\n",
        "    prompt = create_prompt(sample, examples=nShots)\n",
        "\n",
        "    # We generate using generate\n",
        "    prediction = generate(prompt)\n",
        "\n",
        "    return prediction"
      ],
      "metadata": {
        "id": "bwvL1O7u-r9m"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "code = run_custom(\n",
        "    input_text=\"\"\"int main() {\n",
        "    int n;\n",
        "    cin >> n;\n",
        "    int sum = 0;\n",
        "    for (int i = 1; i <= n; i++) {\n",
        "        sum += i;\n",
        "    }\n",
        "    cout << sum << endl;\n",
        "    return 0;\n",
        "}\"\"\",\n",
        "    nShots=5  # He will use 5 examples from the dataset as n-shot\n",
        ")\n",
        "print(code)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVQ2lBHhBYKy",
        "outputId": "28946a3e-b9d2-44a8-ab28-f11b74970a31"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This program reads an integer from the standard input and calculates the sum of all integers from 1 to the given number. It demonstrates basic input-output operations and arithmetic operations in C++.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8a048d8036304b47926cb89c6fdec0d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a125acdb57f8494cbe2959b8cf08411d",
              "IPY_MODEL_36ed13551e8b42e5b8f0ec5cd4413436",
              "IPY_MODEL_339bdb485fe34a8295e16cd2eb87f935"
            ],
            "layout": "IPY_MODEL_9dab811424454f8e9872805b730445e3"
          }
        },
        "a125acdb57f8494cbe2959b8cf08411d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f7f46454e6f433183d33142721c4510",
            "placeholder": "​",
            "style": "IPY_MODEL_4941fde539814ed1909c42744b82fd7b",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "36ed13551e8b42e5b8f0ec5cd4413436": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70b568e72e8d49309344c3b7b354df3e",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_14f42f5df5184e739b7ab36efa4878ce",
            "value": 3
          }
        },
        "339bdb485fe34a8295e16cd2eb87f935": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82fb83a33c1d45f7b1c4ed9e74ba18d1",
            "placeholder": "​",
            "style": "IPY_MODEL_66629080bb564b48aa86d707ba4125ef",
            "value": " 3/3 [01:24&lt;00:00, 28.25s/it]"
          }
        },
        "9dab811424454f8e9872805b730445e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f7f46454e6f433183d33142721c4510": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4941fde539814ed1909c42744b82fd7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "70b568e72e8d49309344c3b7b354df3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14f42f5df5184e739b7ab36efa4878ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "82fb83a33c1d45f7b1c4ed9e74ba18d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66629080bb564b48aa86d707ba4125ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

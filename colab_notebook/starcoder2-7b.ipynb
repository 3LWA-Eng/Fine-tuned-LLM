{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EVX0mlpbs2Zj"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Util code : Restart the kernel\n",
    "# ============================================================================\n",
    "import IPython\n",
    "#IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1766082250691,
     "user": {
      "displayName": "Yossef Moftah",
      "userId": "17869879620553491855"
     },
     "user_tz": -120
    },
    "id": "0-hXpXG3nWrL"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Install Required Libraries\n",
    "# ============================================================================\n",
    "!pip install -q --upgrade bitsandbytes accelerate transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1766082266783,
     "user": {
      "displayName": "Yossef Moftah",
      "userId": "17869879620553491855"
     },
     "user_tz": -120
    },
    "id": "vVn1gG6HnWrN",
    "outputId": "9982b128-3ece-45de-ecd8-da853117be37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['02 Text Preprocessing - Exercise 2.ipynb',\n",
       " 'dataset.csv',\n",
       " 'FeatureExtraction_BoW.ipynb',\n",
       " 'Copy of Fine-tune Llama 2 in Google Colab.ipynb',\n",
       " 'IR_pipeline.ipynb',\n",
       " 'IR_pipeline_colab.ipynb',\n",
       " 'denver_extract.mp3',\n",
       " 'starcoder2-3b.ipynb',\n",
       " 'Dataset.json',\n",
       " 'starcoder2-7b 0.2 .ipynb',\n",
       " 'Output.json']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Import Libraries\n",
    "# ============================================================================\n",
    "import torch\n",
    "import json\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
    "\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "\n",
    "import os\n",
    "# List all notebooks in the folder\n",
    "notebook_folder = '/content/drive/MyDrive/Colab_Notebooks'\n",
    "os.listdir(notebook_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1766082266804,
     "user": {
      "displayName": "Yossef Moftah",
      "userId": "17869879620553491855"
     },
     "user_tz": -120
    },
    "id": "TQq54Tc6Xz6F"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Util Function : Track the consumption of GPU memory\n",
    "# ============================================================================\n",
    "def display_gpu_memory():\n",
    "    \"\"\"Display current GPU memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        # Get memory in bytes and convert to GB\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        free = total - allocated\n",
    "\n",
    "        print(f\"{'='*50}\")\n",
    "        print(f\"GPU Memory Usage:\")\n",
    "        print(f\"{'='*50}\")\n",
    "        print(f\"Allocated: {allocated:.2f} GB\")\n",
    "        print(f\"Reserved:  {reserved:.2f} GB\")\n",
    "        print(f\"Free:      {free:.2f} GB\")\n",
    "        print(f\"Total:     {total:.2f} GB\")\n",
    "        print(f\"Usage:     {(allocated/total)*100:.1f}%\")\n",
    "        print(f\"{'='*50}\")\n",
    "    else:\n",
    "        print(\"CUDA is not available\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1766082266877,
     "user": {
      "displayName": "Yossef Moftah",
      "userId": "17869879620553491855"
     },
     "user_tz": -120
    },
    "id": "KNmLGZixYF9d",
    "outputId": "9be24692-c6e7-4b0b-f3e5-0a5a2edb0b75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "GPU Memory Usage:\n",
      "==================================================\n",
      "Allocated: 0.00 GB\n",
      "Reserved:  0.00 GB\n",
      "Free:      14.74 GB\n",
      "Total:     14.74 GB\n",
      "Usage:     0.0%\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "display_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M0Ln-lxvozr-"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Load StarCoder2 Model\n",
    "# ============================================================================\n",
    "\n",
    "# Quantization Config - this allows us to load the model into memory and use less memory\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # quantization for huge memory saving\n",
    "    bnb_4bit_use_double_quant=True, # double quantization for a bit more memory saving\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # good practice\n",
    "    bnb_4bit_quant_type=\"nf4\" # minor perfromance improvement\n",
    ")\n",
    "\n",
    "starCoder2 = \"bigcode/starcoder2-7b\"\n",
    "model_name = starCoder2\n",
    "quant=True\n",
    "max_new_tokens=80\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model\n",
    "if quant:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quant_config,\n",
    "        device_map=\"auto\"  # Better than .to(\"cuda\") for quantized models\n",
    "    )\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")\n",
    "\n",
    "# Setup streamer\n",
    "#streamer = TextStreamer(tokenizer, skip_special_tokens=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gwWbehIko1Bt"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Define the Prompt Template\n",
    "# ============================================================================\n",
    "SYSTEM_PROMPT = '''\n",
    "Below is an instruction that describes a task, paired with an input that provides further context.\n",
    "Write a response that appropriately completes the request.\n",
    "\n",
    "## Instruction:\n",
    "Summarize the purpose of this C++ code in one or two sentences.\n",
    "\n",
    "## Input:'''\n",
    "\n",
    "ASSISTANT_PRIMER = \"\"\"\n",
    "## Response:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XGnlYWocEE2t"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Generate Function\n",
    "# ============================================================================\n",
    "def generate(code_snippet):\n",
    "  # Format prompt directly for code completion\n",
    "  full_prompt = f\"{SYSTEM_PROMPT}\\n{code_snippet}\\n\\n{ASSISTANT_PRIMER}\"\n",
    "\n",
    "  # For StarCoder2, directly tokenize the prompt (no chat template)\n",
    "  input_ids = tokenizer(full_prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "  attention_mask = torch.ones_like(input_ids, dtype=torch.long, device=\"cuda\")\n",
    "\n",
    "  outputs = model.generate(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask,\n",
    "      max_new_tokens=max_new_tokens,\n",
    "      temperature=0.2,\n",
    "      top_p=0.95,\n",
    "      do_sample=True,  # Required when using temperature\n",
    "      pad_token_id=tokenizer.eos_token_id\n",
    "      #,streamer=streamer\n",
    "  )\n",
    "  full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "  # Extract only the newly generated explanation\n",
    "  prediction = full_text.split(\"Explanation:\")[-1].split(\"---\")[0].strip()\n",
    "\n",
    "  return prediction\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cA8rE_KSpAgm"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Example Usage\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"StarCoder2 Code Analyzer Ready!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Example 1:\n",
    "example_code_1 = \"\"\"\n",
    "#include <iostream>\n",
    "#include <cmath>\n",
    "using namespace std;\n",
    "\n",
    "int main() {\n",
    "    double num = 16.0;\n",
    "    cout << \"Square root of \" << num << \" is \" << sqrt(num) << endl;\n",
    "    cout << \"2 raised to power 3 is \" << pow(2, 3) << endl;\n",
    "    return 0;\n",
    "}\n",
    "\"\"\"\n",
    "#generate(example_code_1)\n",
    "display(Markdown(generate(example_code_1)))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "display_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pHacGJc5fmup"
   },
   "outputs": [],
   "source": [
    "def create_benchmark_prompt(obj):\n",
    "    # Demonstrations teach the BASE model the pattern\n",
    "    few_shot_examples = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\n",
    "Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Summarize the purpose of this C++ code in one or two sentences.\n",
    "\n",
    "### Input:\n",
    "#include <iostream>\n",
    "using namespace std;\n",
    "\n",
    "int main(){\n",
    "    cout << \"Hello world!\\n\";\n",
    "    return 0;\n",
    "}\n",
    "\n",
    "### Response:\n",
    "## This program includes the standard input-output library and prints the message \"Hello world!\" to the console, then terminates successfully.\n",
    "\n",
    "### Instruction:\n",
    "Summarize the purpose of this C++ code in one or two sentences.\n",
    "\n",
    "### Input:\n",
    "#include <iostream>\n",
    "using namespace std;\n",
    "\n",
    "int main(){\n",
    "    int a = 10, b = 20;\n",
    "    int sum = a + b;\n",
    "    cout << \"Sum: \" << sum << endl;\n",
    "    return 0;\n",
    "}\n",
    "\n",
    "### Response:\n",
    "## This program declares two integer variables, adds them together, and displays the result to the console. It demonstrates basic arithmetic operations and output formatting in C++.\n",
    "\n",
    "### Instruction:\n",
    "Summarize the purpose of this C++ code in one or two sentences.\n",
    "\n",
    "### Input:\n",
    "#include <iostream>\n",
    "using namespace std;\n",
    "\n",
    "int main(){\n",
    "    for(int i = 1; i <= 5; i++){\n",
    "        cout << i << \" \";\n",
    "    }\n",
    "    cout << endl;\n",
    "    return 0;\n",
    "}\n",
    "\n",
    "### Response:\n",
    "This program uses a for loop to iterate from 1 to 5 and prints each number separated by spaces. It demonstrates basic loop control structure in C++.\n",
    "\"\"\"\n",
    "\n",
    "    # Your specific test case\n",
    "    test_case = f\"\"\"### Instruction:\n",
    "{obj['instruction']}\n",
    "\n",
    "### Input:\n",
    "{obj['input']}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "    return few_shot_examples + test_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CegViH8kdPnP"
   },
   "outputs": [],
   "source": [
    "# Load Dataset from Drive\n",
    "with open('/content/drive/MyDrive/Colab_Notebooks/Dataset.json', 'r') as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "results = []\n",
    "\n",
    "# 4. Inference Loop\n",
    "for item in dataset:\n",
    "  if(item[\"id\"] > 3): # first 3 data are used in the prompt\n",
    "    prediction = generate(create_benchmark_prompt(item))\n",
    "\n",
    "    results.append({\n",
    "        \"id\": item[\"id\"],\n",
    "        \"input\": item[\"input\"],\n",
    "        \"expected\": item[\"output\"],\n",
    "        \"actual\": prediction\n",
    "    })\n",
    "    print(f\"Processed ID: {item['id']}\")\n",
    "\n",
    "# 5. Save to Drive\n",
    "with open('/content/drive/MyDrive/Colab_Notebooks/Output.json', 'w') as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "print(\"Success! Output.json has been saved to your Drive.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MCw9d3cMpgyY"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# 1. Load the results\n",
    "file_path = '/content/drive/MyDrive/Colab_Notebooks/Output.json'\n",
    "with open(file_path, 'r') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# 2. CLEANING LOGIC: Isolate only the actual response\n",
    "def extract_clean_response(text):\n",
    "    # If the model repeated the prompt, we split at '### Response:'\n",
    "    if \"### Response:\" in text:\n",
    "        text = text.split(\"### Response:\")[-1]\n",
    "\n",
    "    # Remove any trailing \"###\" (common if the model tries to start a new block)\n",
    "    text = text.split(\"###\")[0]\n",
    "\n",
    "    # Remove any line breaks and extra whitespace\n",
    "    return text.strip()\n",
    "\n",
    "# Apply the cleaning to the 'actual' column\n",
    "df['actual'] = df['actual'].apply(extract_clean_response)\n",
    "\n",
    "# 3. Add a Word Count for monitoring conciseness\n",
    "df['word_count'] = df['actual'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# 4. Display the table\n",
    "print(\"--- MODEL BENCHMARK RESULTS (CLEANED) ---\")\n",
    "# Using display(HTML(...)) to make it readable in Colab\n",
    "display(HTML(df[['id', 'expected', 'actual', 'word_count']].to_html()))\n",
    "\n",
    "# 5. Save as CSV\n",
    "csv_path = '/content/drive/MyDrive/Colab_Notebooks/Benchmark_Report_Clean.csv'\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"\\nâœ… Clean report saved to: {csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "https://huggingface.co/bigcode/starcoder2-3b.ipynb",
     "timestamp": 1765461235623
    }
   ]
  },
  "kaggle": {
   "accelerator": "gpu"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
